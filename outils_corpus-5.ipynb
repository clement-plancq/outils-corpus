{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outils-corpus 5\n",
    "## [Spacy](https://spacy.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Biblioth√®que logicielle de TAL √©crite en Python (et Cython)\n",
    "- √âtiquetage POS, lemmatisation, analyse syntaxique, entit√©s nomm√©es, word embedding, transformers\n",
    "- Usage de mod√®les neuronaux\n",
    "- Int√©gration ais√©e de biblioth√®ques de deep learning\n",
    "- v3.0.3 ([github](https://github.com/explosion/spaCy))\n",
    "- Licence MIT (Open Source) pour le code\n",
    "    - Licences ouvertes diverses pour les mod√®les\n",
    "- Produit de la soci√©t√© [explosion.ai](https://explosion.ai/). Fond√© par :¬†Matthew Honnibal ([@honnibal](https://twitter.com/honnibal)) et Ines Montani ([@_inesmontani](https://twitter.com/_inesmontani))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pourquoi Spacy ?\n",
    "\n",
    "- C'est du Python üôå üéâ\n",
    "- Plut√¥t simple √† prendre en main\n",
    "- Tr√®s bien document√©, √† notre avis. D'ailleurs plut√¥t que ce notebook, suivez l'excellent tutorial d'Ines Montani : [https://course.spacy.io/](https://course.spacy.io/)\n",
    "- Couvre les traitements d'une cha√Æne de TAL typique\n",
    "- Pas mal utilis√© dans l'industrie\n",
    "- MAIS ce n'est pas forc√©ment l'outil qui donne les meilleurs r√©sultats pour le fran√ßais dans toutes les t√¢ches de TAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy et les autres\n",
    "\n",
    "Spacy est *un* des frameworks de TAL disponibles\n",
    "\n",
    "- [NLTK](http://www.nltk.org/) :¬†python, orient√© p√©dagogie, pas de mod√®les neuronaux inclus mais se combine bien avec TensorFlow, PyTorch ou AlleNLP\n",
    "- [Stanford Core¬†NLP](https://stanfordnlp.github.io/stanfordnlp/) :¬†java, mod√®les pour 53 langues (UD), r√©solution de la cor√©ference.\n",
    "- [Stanza](https://stanfordnlp.github.io/stanza/) :¬†python, nouveau framework de Stanford, mod√®les neuronaux entra√Æn√©s sur donn√©es UD <small>[https://github.com/explosion/spacy-stanza](https://github.com/explosion/spacy-stanza) permet d'utiliser les mod√®les de Stanford avec Spacy</small>\n",
    "- [TextBlob](https://textblob.readthedocs.io/en/dev/)\n",
    "- [DKPro](https://dkpro.github.io/)\n",
    "- [flair](https://github.com/zalandoresearch/flair) : le framework de Zalando, tr√®s bonnes performances en reconnaissance d'entit√©s nomm√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## installation\n",
    "\n",
    "dans un terminal\n",
    "```bash\n",
    "python3 -m pip install -U --user spacy \n",
    "#ou pip install -U --user spacy\n",
    "```\n",
    "- installation du mod√®le fran√ßais\n",
    "```bash\n",
    "python3 -m spacy download fr_core_news_md\n",
    "#ou python3 -m spacy download fr_core_news_sm \n",
    "```\n",
    "- v√©rification\n",
    "```bash\n",
    "python3 -m spacy validate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mod√®les\n",
    "\n",
    "- Spacy utilise des mod√®les statistiques qui permettent de pr√©dire des annotations linguistiques\n",
    "- 16 langues :¬†allemand, anglais, chinois, danois, espagnol, fran√ßais, italien, japonais, lituanien, n√©erlandais, grec, norv√©gien, polonais, portugais, roumain, russe + mod√®le multi langues\n",
    "- 4 mod√®les pour le fran√ßais\n",
    "    - fr_core_news_sm (tagger, morphologizer, lemmatizer, parser, ner) 16 Mo\n",
    "    - fr_core_news_md (tagger, morphologizer, lemmatizer, parser, ner, vectors) 45 Mo\n",
    "    - fr_core_news_lg (tagger, morphologizer, lemmatizer, parser, ner, vectors) 546 Mo\n",
    "    - fr_dep_news_trf (tagger, morphologizer, lemmatizer, parser) 381 Mo\n",
    "- mod√®les `fr` appris sur les corpus [Sequoia](https://deep-sequoia.inria.fr/fr/) et [WikiNer](https://figshare.com/articles/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500) sauf le mod√®le `trf` qui est issu de camembert-base distribu√© par [Hugging Face](https://huggingface.co/camembert-base).\n",
    "- Tous ces mod√®les, quelque soient leur type ou leur langue, s'utilisent de la m√™me fa√ßon, avec la m√™me API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage\n",
    "\n",
    "- *si vous voulez utiliser Spacy prenez le temps de lire la [documentation](https://spacy.io/usage), ici ce ne sera qu'un coup d'≈ìil incomplet*\n",
    "- un mod√®le est une instance de la classe `Language`, il est adapt√© √† une langue en particulier\n",
    "- un mod√®le incorpore un vocabulaire, des poids, des vecteurs de mots, une configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.fr.French"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- le traitement fonctionne avec un [*pipeline*](https://spacy.io/usage/spacy-101#pipelines) pour convertir un texte en objet `Doc` (texte annot√©)\n",
    "- par d√©faut `tokenizer` > `tagger` > `parser` > `ner` > `‚Ä¶`\n",
    "- depuis la v3 le pipeline devient `tok2vec` > `morphologizer` > `parser` > `ner` > `attribute_ruler` > `lemmatizer`  \n",
    "  ou `transformer` > `morphologizer` > `parser` > `ner` > `attribute_ruler` > `lemmatizer`\n",
    "- l'utilisateur peut ajouter des √©tapes ou en retrancher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f4854fb5a10>),\n",
       " ('morphologizer',\n",
       "  <spacy.pipeline.morphologizer.Morphologizer at 0x7f4854f4ffb0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f4854f1d7d0>),\n",
       " ('lemmatizer', <spacy.lang.fr.lemmatizer.FrenchLemmatizer at 0x7f4854f18fa0>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('fr_core_news_md', disable=[\"parser\", \"ner\"])\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retour au pipeline par d√©faut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f4854f933b0>),\n",
       " ('morphologizer',\n",
       "  <spacy.pipeline.morphologizer.Morphologizer at 0x7f4860c4e710>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f486273d600>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f486273d520>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f4854f254b0>),\n",
       " ('lemmatizer', <spacy.lang.fr.lemmatizer.FrenchLemmatizer at 0x7f4847288a50>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('fr_core_news_md')\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Un objet `Doc` est une s√©quence d'objets `Token` (voir l'[API](https://spacy.io/api/token))\n",
    " - Le texte d'origine est d√©coup√© en phrases, tokeniz√©, annot√© en POS, lemme, syntaxe (d√©pendance) et en entit√©s nomm√©es (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"L‚ÄôOrganisation des Nations unies (ONU) a lanc√© mardi un appel d‚Äôurgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus.\")\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì tokenization\n",
    "\n",
    "La tokenization de Spacy est non-destructive. Vous pouvez d√©couper un texte en tokens et le restituer dans sa forme originale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'\n",
      "Organisation\n",
      "des\n",
      "Nations\n",
      "unies\n",
      "(\n",
      "ONU\n",
      ")\n",
      "a\n",
      "lanc√©\n",
      "mardi\n",
      "un\n",
      "appel\n",
      "d'\n",
      "urgence\n",
      "pour\n",
      "lever\n",
      "des\n",
      "dizaines\n",
      "de\n",
      "millions\n",
      "de\n",
      "dollars\n",
      "afin\n",
      "de\n",
      "prot√©ger\n",
      "les\n",
      "r√©fugi√©s\n",
      "vuln√©rables\n",
      "face\n",
      "√†\n",
      "la\n",
      "propagation\n",
      "du\n",
      "nouveau\n",
      "coronavirus\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"L'Organisation des Nations unies (ONU) a lanc√© mardi un appel d'urgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus.\")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'Organisation des Nations unies (ONU) a lanc√© mardi un appel d'urgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus."
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text_with_ws, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì √©tiquetage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les annotations portant sur les tokens sont accessibles via les attributs des objets de type `token`‚ÄØ: [https://spacy.io/api/token#attributes](https://spacy.io/api/token#attributes)  \n",
    "  - `pos_` contient l'√©tiquette de partie du discours de [universal dependancies](https://universaldependencies.org/docs/u/pos/)\n",
    "  - `tag_` contient l'√©tiquette du corpus original, parfois plus d√©taill√©e\n",
    "  - `lemma_` pour le lemme\n",
    "  - `morph` pour l'analyse morphologique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L' DET Definite=Def|Number=Sing|PronType=Art le\n",
      "Organisation NOUN Gender=Fem|Number=Sing organisation\n",
      "des ADP Definite=Def|Number=Plur|PronType=Art de\n",
      "Nations PROPN  Nations\n",
      "unies ADJ Gender=Fem|Number=Plur uni\n",
      "( PUNCT  (\n",
      "ONU PROPN Gender=Fem|Number=Sing ONU\n",
      ") PUNCT  )\n",
      "a AUX Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin avoir\n",
      "lanc√© VERB Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part lancer\n",
      "mardi NOUN Gender=Masc|Number=Sing mardi\n",
      "un DET Definite=Ind|Gender=Masc|Number=Sing|PronType=Art un\n",
      "appel NOUN Gender=Masc|Number=Sing appel\n",
      "d' ADP  de\n",
      "urgence NOUN Gender=Fem|Number=Sing urgence\n",
      "pour ADP  pour\n",
      "lever VERB VerbForm=Inf lever\n",
      "des DET Definite=Ind|Number=Plur|PronType=Art un\n",
      "dizaines NOUN Gender=Fem|Number=Plur dizaine\n",
      "de ADP  de\n",
      "millions NOUN Gender=Masc|NumType=Card|Number=Plur million\n",
      "de ADP  de\n",
      "dollars NOUN Gender=Masc|Number=Plur dollar\n",
      "afin ADV  afin\n",
      "de ADP  de\n",
      "prot√©ger VERB VerbForm=Inf prot√©ger\n",
      "les DET Definite=Def|Number=Plur|PronType=Art le\n",
      "r√©fugi√©s NOUN Gender=Masc|Number=Plur r√©fugi√©\n",
      "vuln√©rables ADJ Number=Plur vuln√©rable\n",
      "face NOUN Gender=Fem|Number=Sing face\n",
      "√† ADP  √†\n",
      "la DET Definite=Def|Gender=Fem|Number=Sing|PronType=Art le\n",
      "propagation NOUN Gender=Fem|Number=Sing propagation\n",
      "du ADP Definite=Def|Gender=Masc|Number=Sing|PronType=Art de\n",
      "nouveau ADJ Gender=Masc|Number=Sing nouveau\n",
      "coronavirus NOUN Gender=Masc|Number=Sing coronavirus\n",
      ". PUNCT  .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.morph, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour traiter plusieurs textes en s√©rie, il est recommand√© d'utiliser [nlp.pipe](https://spacy.io/api/language#pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Cadine avait un tr√®s-mauvais caract√®re. Elle ne s‚Äôaccommodait pas du r√¥le de servante.\",\n",
    "    \"Aussi finit-elle par s‚Äô√©tablir pour son compte.\",\n",
    "    \"Comme elle √©tait alors √¢g√©e de treize ans, et qu‚Äôelle ne pouvait r√™ver le grand commerce, un banc de vente de l‚Äôall√©e aux fleurs, elle vendit des bouquets de violettes d‚Äôun sou, piqu√©s dans un lit de mousse, sur un √©ventaire d‚Äôosier pendu √† son cou.\",\n",
    "    \"Elle r√¥dait toute la journ√©e dans les Halles, autour des Halles, promenant son bout de pelouse.\",\n",
    "    \"C‚Äô√©tait l√† sa joie, cette fl√¢nerie continuelle, qui lui d√©gourdissait les jambes, qui la tirait des longues heures pass√©es √† faire des bouquets, les genoux pli√©s, sur une chaise basse.\",\n",
    "    \"Maintenant, elle tournait ses violettes en marchant, elle les tournait comme des fuseaux, avec une merveilleuse l√©g√®ret√© de doigts ; elle comptait six √† huit fleurs, selon la saison, pliait en deux un brin de jonc, ajoutait une feuille, roulait un fil mouill√© ; et, entre ses dents de jeune loup, elle cassait le fil.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp.pipe(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä¬†vous  \n",
    "1. Extrayez de la s√©rie de phrases ci-dessus la liste des noms communs\n",
    "2. Comptez le nombre de tokens au masculin et au f√©minin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chaise, commerce, fil, banc, bouquets, violettes, fil, fuseaux, mousse, r√¥le, compte, bout, heures, osier, saison, brin, ans, feuille, vente, all√©e, violettes, -, sou, l√©g√®ret√©, servante, Halles, pelouse, jambes, bouquets, fleurs, genoux, dents, fleurs, loup, Cadine, caract√®re, joie, journ√©e, fl√¢nerie, lit, √©ventaire, Halles, doigts, cou\n"
     ]
    }
   ],
   "source": [
    "# Extrayez de la s√©rie de phrases ci-dessus la liste des noms communs\n",
    "\n",
    "ncs = []\n",
    "docs = nlp.pipe(texts)\n",
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            ncs.append(token)\n",
    "\n",
    "ncs_set = set(ncs)\n",
    "#for item in ncs:\n",
    "#    print(item.text)\n",
    "print(\", \".join([item.text for item in ncs_set]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens au masculin : 39, tokens au f√©minin : 47\n"
     ]
    }
   ],
   "source": [
    "# Comptez le nombre de tokens au masculin et au f√©minin\n",
    "\n",
    "nb_masc = 0\n",
    "nb_fem = 0\n",
    "\n",
    "docs = nlp.pipe(texts)\n",
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        if token.morph.get(\"Gender\") == [\"Masc\"]:\n",
    "            nb_masc += 1\n",
    "        elif token.morph.get(\"Gender\") == [\"Fem\"]:\n",
    "            nb_fem += 1\n",
    "            \n",
    "print(f\"tokens au masculin : {nb_masc}, tokens au f√©minin : {nb_fem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si NER (*Named Entity Recognition*) fait partie de votre mod√®le, vos donn√©es seront annot√©es √©galement en entit√©s nomm√©es.  \n",
    "Vous pouvez y acc√©der avec l'attribut `ent_type_` des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L' \n",
      "Organisation ORG\n",
      "des ORG\n",
      "Nations ORG\n",
      "unies ORG\n",
      "( \n",
      "ONU ORG\n",
      ") \n",
      "a \n",
      "lanc√© \n",
      "mardi \n",
      "un \n",
      "appel \n",
      "d‚Äô \n",
      "urgence \n",
      "pour \n",
      "lever \n",
      "des \n",
      "dizaines \n",
      "de \n",
      "millions \n",
      "de \n",
      "dollars \n",
      "afin \n",
      "de \n",
      "prot√©ger \n",
      "les \n",
      "r√©fugi√©s \n",
      "vuln√©rables \n",
      "face \n",
      "√† \n",
      "la \n",
      "propagation \n",
      "du \n",
      "nouveau \n",
      "coronavirus \n",
      ". \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"L'Organisation des Nations unies (ONU) a lanc√© mardi un appel d‚Äôurgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus.\")\n",
    "for token in doc:\n",
    "    print(token, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou acc√©der directement aux entit√©s de l'objet `Doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organisation des Nations unies ORG\n",
      "ONU ORG\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "doc = nlp(\"L‚ÄôOrganisation des Nations unies (ONU) a lanc√© mardi un appel d‚Äôurgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy int√®gre un outil de visualisation pour l'annotation en entit√©s nomm√©es :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">L'\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Organisation des Nations unies\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ONU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ") a lanc√© mardi un appel d‚Äôurgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Le pr√©sident \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Xi Jinping\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " a affirm√© que la propagation du coronavirus √©tait ¬´‚ÄØpratiquement jugul√©e‚ÄØ¬ª. Il s‚Äôest d‚Äôailleurs rendu pour la premi√®re fois √† \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wuhan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", la capitale de la province du \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Hubei\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", le berceau du \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Covid-19\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp('Le pr√©sident Xi Jinping a affirm√© que la propagation du coronavirus √©tait ¬´‚ÄØpratiquement jugul√©e‚ÄØ¬ª. Il s‚Äôest d‚Äôailleurs rendu pour la premi√®re fois √† Wuhan, la capitale de la province du Hubei, le berceau du Covid-19.')\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Derri√®re lui, sur le carreau de la \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    rue Rambuteau\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", on vendait des fruits.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"Derri√®re lui, sur le carreau de la rue Rambuteau, on vendait des fruits.\")\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä¬†vous  \n",
    "\n",
    "Dans `files/Le_Ventre_de_Paris-short.txt` (ou un texte de votre choix), comptez la fr√©quence de chaque entit√© de type PER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Florent', 218), ('Lisa', 76), ('Claude', 33), ('Pauline', 22), ('Saget', 18), ('Lec≈ìur', 18), ('Auguste', 18), ('Augustine', 16), ('Fran√ßois', 15), ('Charvet', 15), ('Lebigre', 13), ('L√©on', 13), ('Robine', 11), ('M√©hudin', 10), ('madame Quenu', 9), ('Gradelle', 8), ('Cl√©mence', 8), ('Balthazar', 6), ('Alexandre', 6), ('Madame Fran√ßois', 5), ('Chantemesse', 5), ('Jules', 5), ('Marjolin', 5), ('Claude Lantier', 5), ('Quenu', 5), ('Collard', 5), ('Louise', 5), ('Claire', 5), ('Murillo', 4), ('Macquart', 4), ('Mademoiselle Saget', 4), ('Verlaque', 4), ('Hein', 3), ('Voyons', 3), ('Cadine', 3), ('Madame Lec≈ìur', 3), ('Gavard', 3), ('Rose', 3), ('Augustine Landois', 2), ('Charles X', 2), ('Louis-Philippe', 2), ('Madame Taboureau', 2), ('Mouton', 2), ('Doucement', 2), ('√âchapp√© de Cayenne', 1), ('Marcel', 1), ('Dis Marcel', 1), ('Lacaille', 1), ('Lundi', 1), ('Adieu', 1), ('Cendrillon', 1), ('Rubens', 1), ('Guillout', 1), ('Aveugl√©', 1), ('de Florent', 1), ('Cuvier', 1), ('Gavard voulut', 1), ('Gutenberg', 1), ('Pardieu', 1), ('Florent Laquerri√®re', 1), ('Auguste Landois', 1), ('Saint-Ouen', 1), ('Napol√©on III', 1), ('Morny', 1), ('Monsieur', 1), ('M√©nilmontant', 1), ('√âcoute', 1), ('Jean-Jacques Rousseau', 1), ('Louise M√©hudin', 1), ('Allez', 1), ('madame Taboureau', 1), ('Quarante', 1), ('Madame Robine', 1), ('Monsieur Manoury', 1), ('Gavard pr√©senta Florent', 1), ('Manoury', 1), ('Logre', 1), ('la Vierge', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "personnes = Counter()\n",
    "with open('files/Le_Ventre_de_Paris-short.txt') as input_data:\n",
    "    docs = nlp.pipe(input_data)\n",
    "    for doc in docs:\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PER\":\n",
    "                personnes[ent.text] += 1\n",
    "\n",
    "print(personnes.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì analyse syntaxique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analyse syntaxique ou *parsing* de Spacy est une analyse en d√©pendance. La plupart sinon la totalit√© des mod√®les utilis√©s viennent de https://universaldependencies.org\n",
    "\n",
    "Dans l'analyse en d√©pendance produite par Spacy, chaque mot d'une phrase a un gouverneur unique (*head*), la relation de d√©pendance entre le mot et son gouverneur est typ√©e (*nsubj*, *obj*, ‚Ä¶).  \n",
    "Pour la t√™te de la phrase on utilise la relation *ROOT*.\n",
    "\n",
    "La structure produite par l'analyse syntaxique est un arbre, un graphe acyclique et connexe. Les tokens sont les n≈ìuds, les arcs sont les d√©pendances, le type de la relation est l'√©tiquette de l'arc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`displacy` fournit un outil de visualisation bien pratique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"fr\" id=\"4abe6432a18348afa1a8c82b2ebe2189-0\" class=\"displacy\" width=\"1220\" height=\"362.0\" direction=\"ltr\" style=\"max-width: none; height: 362.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Derri√®re</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">lui,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">sur</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">le</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">carreau</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">de</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">la</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">rue</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">Rambuteau,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">vendait</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\">des</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">fruits.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4abe6432a18348afa1a8c82b2ebe2189-0-0\" stroke-width=\"2px\" d=\"M70,227.0 C70,182.0 120.0,182.0 120.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4abe6432a18348afa1a8c82b2ebe2189-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,229.0 L62,217.0 78,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4abe6432a18348afa1a8c82b2ebe2189-0-1\" stroke-width=\"2px\" d=\"M160,227.0 C160,2.0 950.0,2.0 950.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4abe6432a18348afa1a8c82b2ebe2189-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl:mod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M160,229.0 L152,217.0 168,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4abe6432a18348afa1a8c82b2ebe2189-0-2\" stroke-width=\"2px\" d=\"M250,227.0 C250,137.0 395.0,137.0 395.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4abe6432a18348afa1a8c82b2ebe2189-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M250,229.0 L242,217.0 258,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4abe6432a18348afa1a8c82b2ebe2189-0-3\" stroke-width=\"2px\" d=\"M340,227.0 C340,182.0 390.0,182.0 390.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4abe6432a18348afa1a8c82b2ebe2189-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M340,229.0 L332,217.0 348,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4abe6432a18348afa1a8c82b2ebe2189-0-4\" stroke-width=\"2px\" d=\"M430,227.0 C430,47.0 945.0,47.0 945.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4abe6432a18348afa1a8c82b2ebe2189-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl:mod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M430,229.0 L422,217.0 438,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4abe6432a18348afa1a8c82b2ebe2189-0-5\" stroke-width=\"2px\" d=\"M520,227.0 C520,137.0 665.0,137.0 665.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4abe6432a18348afa1a8c82b2ebe2189-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M520,229.0 L512,217.0 528,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4abe6432a18348afa1a8c82b2ebe2189-0-6\" stroke-width=\"2px\" d=\"M610,227.0 C610,182.0 660.0,182.0 660.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4abe6432a18348afa1a8c82b2ebe2189-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M610,229.0 L602,217.0 618,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4abe6432a18348afa1a8c82b2ebe2189-0-7\" stroke-width=\"2px\" d=\"M430,227.0 C430,92.0 670.0,92.0 670.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4abe6432a18348afa1a8c82b2ebe2189-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670.0,229.0 L678.0,217.0 662.0,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4abe6432a18348afa1a8c82b2ebe2189-0-8\" stroke-width=\"2px\" d=\"M700,227.0 C700,182.0 750.0,182.0 750.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4abe6432a18348afa1a8c82b2ebe2189-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,229.0 L758.0,217.0 742.0,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4abe6432a18348afa1a8c82b2ebe2189-0-9\" stroke-width=\"2px\" d=\"M880,227.0 C880,182.0 930.0,182.0 930.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4abe6432a18348afa1a8c82b2ebe2189-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M880,229.0 L872,217.0 888,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4abe6432a18348afa1a8c82b2ebe2189-0-10\" stroke-width=\"2px\" d=\"M1060,227.0 C1060,182.0 1110.0,182.0 1110.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4abe6432a18348afa1a8c82b2ebe2189-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1060,229.0 L1052,217.0 1068,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4abe6432a18348afa1a8c82b2ebe2189-0-11\" stroke-width=\"2px\" d=\"M970,227.0 C970,137.0 1115.0,137.0 1115.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4abe6432a18348afa1a8c82b2ebe2189-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1115.0,229.0 L1123.0,217.0 1107.0,217.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"Derri√®re lui, sur le carreau de la rue Rambuteau, on vendait des fruits.\")\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options={'distance':90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe √©galement un outil issu d'un d√©veloppement ind√©pendant :¬†[explacy](https://spacy.io/universe/project/explacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dep tree     Token     Dep type Lemma     Part of Sp\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "         ‚îå‚îÄ‚ñ∫ Derri√®re  case     derri√®re  ADP       \n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îî‚îÄ‚îÄ lui       obl:mod  lui       PRON      \n",
      "‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ,         punct    ,         PUNCT     \n",
      "‚îÇ‚îÇ      ‚îå‚îÄ‚îÄ‚ñ∫ sur       case     sur       ADP       \n",
      "‚îÇ‚îÇ      ‚îÇ‚îå‚îÄ‚ñ∫ le        det      le        DET       \n",
      "‚îÇ‚îÇ‚îå‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚î¥‚î¥‚îÄ‚îÄ carreau   obl:mod  carreau   NOUN      \n",
      "‚îÇ‚îÇ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚ñ∫ de        case     de        ADP       \n",
      "‚îÇ‚îÇ‚îÇ  ‚îÇ  ‚îÇ‚îå‚îÄ‚ñ∫ la        det      le        DET       \n",
      "‚îÇ‚îÇ‚îÇ  ‚îî‚îÄ‚ñ∫‚îî‚îº‚îÄ‚îÄ rue       nmod     rue       NOUN      \n",
      "‚îÇ‚îÇ‚îÇ      ‚îî‚îÄ‚ñ∫ Rambuteau nmod     Rambuteau PROPN     \n",
      "‚îÇ‚îÇ‚îÇ     ‚îå‚îÄ‚îÄ‚ñ∫ ,         punct    ,         PUNCT     \n",
      "‚îÇ‚îÇ‚îÇ     ‚îÇ‚îå‚îÄ‚ñ∫ on        nsubj    on        PRON      \n",
      "‚îî‚î¥‚î¥‚îÄ‚îÄ‚î¨‚î¨‚îÄ‚î¥‚î¥‚îÄ‚îÄ vendait   ROOT     vendre    VERB      \n",
      "     ‚îÇ‚îÇ  ‚îå‚îÄ‚ñ∫ des       det      un        DET       \n",
      "     ‚îÇ‚îî‚îÄ‚ñ∫‚îî‚îÄ‚îÄ fruits    obj      fruit     NOUN      \n",
      "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ .         punct    .         PUNCT     \n"
     ]
    }
   ],
   "source": [
    "import explacy\n",
    "explacy.print_parse_info(nlp, 'Derri√®re lui, sur le carreau de la rue Rambuteau, on vendait des fruits.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi r√©cup√©rer parcourir les tokens et afficher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maintenant ADVMOD tournait\n",
      ", PUNCT tournait\n",
      "elle NSUBJ tournait\n",
      "tournait ROOT tournait\n",
      "ses DET violettes\n",
      "violettes OBJ tournait\n",
      "en CASE marchant\n",
      "marchant ADVCL tournait\n",
      ", PUNCT tournait\n",
      "elle NSUBJ tournait\n",
      "les OBJ tournait\n",
      "tournait CONJ tournait\n",
      "comme CASE fuseaux\n",
      "des DET fuseaux\n",
      "fuseaux OBL:MOD tournait\n",
      ", PUNCT tournait\n",
      "avec CASE l√©g√®ret√©\n",
      "une DET l√©g√®ret√©\n",
      "merveilleuse AMOD l√©g√®ret√©\n",
      "l√©g√®ret√© OBL:MOD tournait\n",
      "de CASE doigts\n",
      "doigts NMOD l√©g√®ret√©\n",
      "; PUNCT tournait\n",
      "elle NSUBJ comptait\n",
      "comptait ROOT comptait\n",
      "six NUMMOD fleurs\n",
      "√† CASE huit\n",
      "huit NMOD six\n",
      "fleurs OBJ comptait\n",
      ", PUNCT comptait\n",
      "selon CASE saison\n",
      "la DET saison\n",
      "saison OBL:MOD comptait\n",
      ", PUNCT pliait\n",
      "pliait CONJ comptait\n",
      "en CASE deux\n",
      "deux OBL:MOD pliait\n",
      "un DET brin\n",
      "brin OBJ pliait\n",
      "de CASE jonc\n",
      "jonc NMOD brin\n",
      ", PUNCT ajoutait\n",
      "ajoutait CONJ comptait\n",
      "une DET feuille\n",
      "feuille OBJ ajoutait\n",
      ", PUNCT roulait\n",
      "roulait CONJ comptait\n",
      "un DET fil\n",
      "fil OBJ roulait\n",
      "mouill√© AMOD fil\n",
      "; PUNCT comptait\n",
      "et CC cassait\n",
      ", PUNCT cassait\n",
      "entre CASE dents\n",
      "ses DET dents\n",
      "dents OBL:MOD cassait\n",
      "de CASE loup\n",
      "jeune AMOD loup\n",
      "loup NMOD dents\n",
      ", PUNCT cassait\n",
      "elle NSUBJ cassait\n",
      "cassait ROOT cassait\n",
      "le DET fil\n",
      "fil OBJ cassait\n",
      ". PUNCT cassait\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.dep_.upper(), token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les attributs de token suivant peuvent √™tre utilis√©s pour parcourir l'arbre de d√©pendance :¬†\n",
    "- `children` les tokens d√©pendants du token\n",
    "- `subtree` tous les descendants du token\n",
    "- `ancestors` tous les parents du token\n",
    "- `rights` les enfants √† droite du token\n",
    "- `lefts` les enfants √† gauche du token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut extraire de la phrase pr√©c√©dente le triplet sujet-verbe-objet comme ceci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(on, vendait, fruits)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = [token for token in doc if token.head == token][0]\n",
    "subjects = [tok for tok in root.lefts if tok.dep_ == \"nsubj\"]\n",
    "subject = subjects[0]\n",
    "objs = [tok for tok in root.rights if tok.dep_ == \"obj\"]\n",
    "obj = objs[0]\n",
    "subject, root, obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "des\n",
      "fruits\n"
     ]
    }
   ],
   "source": [
    "for obj in objs:\n",
    "    for descendant in obj.subtree:\n",
    "        print(descendant.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä¬†vous\n",
    "\n",
    "1. Trouver et afficher l'objet de la phrase :¬†¬´ Depuis que Google a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle. ¬ª\n",
    "\n",
    "2. Dans la liste `texts` vue plus haut, retrouvez les verbes dont Cadine ou le pronom 'elle' est sujet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dep tree                         Token      Dep type   Lemma      Part of Sp\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Depuis     mark       depuis     ADP       \n",
      "                          ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚ñ∫ que        mark       que        SCONJ     \n",
      "                          ‚îÇ‚îÇ‚îå‚îÄ‚îÄ‚ñ∫ Google     nsubj      Google     PROPN     \n",
      "                          ‚îÇ‚îÇ‚îÇ‚îå‚îÄ‚ñ∫ a          aux:tense  avoir      AUX       \n",
      "‚îå‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚î¥‚î¥‚î¥‚îÄ‚îÄ annonc√©    advcl      annoncer   VERB      \n",
      "‚îÇ  ‚îÇ                         ‚îå‚îÄ‚ñ∫ son        det        son        DET       \n",
      "‚îÇ  ‚îî‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ intention  obj        intention  NOUN      \n",
      "‚îÇ     ‚îÇ                      ‚îå‚îÄ‚ñ∫ de         mark       de         ADP       \n",
      "‚îÇ     ‚îî‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ stopper    acl        stopper    VERB      \n",
      "‚îÇ        ‚îÇ                ‚îå‚îÄ‚ñ∫‚îå‚îÄ‚îÄ d'         advmod     de         ADP       \n",
      "‚îÇ        ‚îÇ                ‚îÇ  ‚îî‚îÄ‚ñ∫ ici        fixed      ici        ADV       \n",
      "‚îÇ        ‚îÇ                ‚îÇ  ‚îå‚îÄ‚ñ∫ deux       nummod     deux       NUM       \n",
      "‚îÇ        ‚îÇ             ‚îå‚îÄ‚ñ∫‚îî‚îÄ‚îÄ‚î¥‚îÄ‚îÄ ans        nmod       an         NOUN      \n",
      "‚îÇ        ‚îÇ             ‚îÇ     ‚îå‚îÄ‚ñ∫ les        det        le         DET       \n",
      "‚îÇ        ‚îî‚îÄ‚ñ∫‚îå‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îº‚îÄ‚îÄ cookies    obj        cookie     NOUN      \n",
      "‚îÇ           ‚îÇ‚îÇ            ‚îÇ  ‚îî‚îÄ‚ñ∫ tiers      amod       tiers      ADJ       \n",
      "‚îÇ           ‚îÇ‚îÇ            ‚îÇ  ‚îå‚îÄ‚ñ∫ sur        case       sur        ADP       \n",
      "‚îÇ           ‚îÇ‚îÇ            ‚îî‚îÄ‚ñ∫‚îî‚îÄ‚îÄ Chrome     nmod       Chrome     PROPN     \n",
      "‚îÇ           ‚îÇ‚îÇ              ‚îå‚îÄ‚îÄ‚ñ∫ ,          punct      ,          PUNCT     \n",
      "‚îÇ           ‚îÇ‚îÇ              ‚îÇ‚îå‚îÄ‚ñ∫ son        det        son        DET       \n",
      "‚îÇ           ‚îÇ‚îî‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚î¥‚î¥‚îÄ‚îÄ moteur     conj       moteur     NOUN      \n",
      "‚îÇ           ‚îÇ   ‚îÇ         ‚îÇ  ‚îå‚îÄ‚ñ∫ de         case       de         ADP       \n",
      "‚îÇ           ‚îÇ   ‚îÇ         ‚îî‚îÄ‚ñ∫‚îî‚îÄ‚îÄ recherche  nmod       recherche  NOUN      \n",
      "‚îÇ           ‚îÇ   ‚îÇ           ‚îå‚îÄ‚îÄ‚ñ∫ qui        nsubj:pass qui        PRON      \n",
      "‚îÇ           ‚îÇ   ‚îÇ           ‚îÇ‚îå‚îÄ‚ñ∫ est        aux:pass   √™tre       AUX       \n",
      "‚îÇ           ‚îÇ   ‚îî‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚î¥‚îÄ‚îÄ utilis√©    acl:relcl  utiliser   VERB      \n",
      "‚îÇ           ‚îÇ      ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ par        case       par        ADP       \n",
      "‚îÇ           ‚îÇ      ‚îÇ  ‚îÇ   ‚îå‚îÄ‚ñ∫‚îå‚îÄ‚îÄ plus       advmod     plus       ADV       \n",
      "‚îÇ           ‚îÇ      ‚îÇ  ‚îÇ   ‚îÇ  ‚îî‚îÄ‚ñ∫ de         fixed      de         ADP       \n",
      "‚îÇ           ‚îÇ      ‚îÇ  ‚îÇ‚îå‚îÄ‚ñ∫‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 60         nummod     60         NUM       \n",
      "‚îÇ           ‚îÇ      ‚îî‚îÄ‚ñ∫‚îî‚î¥‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ %          obl:agent  pourcent   NOUN      \n",
      "‚îÇ           ‚îÇ            ‚îÇ  ‚îå‚îÄ‚îÄ‚ñ∫ de         case       de         ADP       \n",
      "‚îÇ           ‚îÇ            ‚îÇ  ‚îÇ‚îå‚îÄ‚ñ∫ la         det        le         DET       \n",
      "‚îÇ           ‚îÇ            ‚îî‚îÄ‚ñ∫‚îú‚îº‚îÄ‚îÄ population nmod       population NOUN      \n",
      "‚îÇ           ‚îÇ               ‚îÇ‚îî‚îÄ‚ñ∫ mondiale   amod       mondial    ADJ       \n",
      "‚îÇ           ‚îÇ               ‚îî‚îÄ‚îÄ‚ñ∫ connect√©e  acl        connecter  VERB      \n",
      "‚îÇ           ‚îÇ               ‚îå‚îÄ‚îÄ‚ñ∫ ,          punct      ,          PUNCT     \n",
      "‚îÇ           ‚îÇ               ‚îÇ‚îå‚îÄ‚ñ∫ les        det        le         DET       \n",
      "‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îå‚î¨‚îÄ‚î¥‚î¥‚îÄ‚îÄ Criteo     conj       Criteo     PROPN     \n",
      "‚îÇ                        ‚îÇ‚îÇ  ‚îå‚îÄ‚ñ∫ ,          punct      ,          PUNCT     \n",
      "‚îÇ                        ‚îÇ‚îî‚îÄ‚ñ∫‚îî‚îÄ‚îÄ LiveRamp   conj       LiveRamp   PROPN     \n",
      "‚îÇ                        ‚îÇ  ‚îå‚îÄ‚îÄ‚ñ∫ et         cc         et         CCONJ     \n",
      "‚îÇ                        ‚îÇ  ‚îÇ‚îå‚îÄ‚ñ∫ autres     amod       autre      ADJ       \n",
      "‚îÇ                        ‚îî‚îÄ‚ñ∫‚îî‚îº‚îÄ‚îÄ Index      conj       index      NOUN      \n",
      "‚îÇ                            ‚îî‚îÄ‚ñ∫ Exchange   nmod       Exchange   PROPN     \n",
      "‚îÇ                            ‚îå‚îÄ‚ñ∫ se         expl:comp  se         PRON      \n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ pr√©parent  ROOT       pr√©parer   VERB      \n",
      "             ‚îÇ‚îÇ              ‚îå‚îÄ‚ñ∫ √†          case       √†          ADP       \n",
      "             ‚îÇ‚îî‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ ce         obl:arg    ce         PRON      \n",
      "             ‚îÇ   ‚îÇ           ‚îå‚îÄ‚ñ∫ qui        nsubj      qui        PRON      \n",
      "             ‚îÇ   ‚îî‚îÄ‚ñ∫‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ peut       acl:relcl  pouvoir    VERB      \n",
      "             ‚îÇ      ‚îÇ        ‚îå‚îÄ‚ñ∫ √™tre       aux:pass   √™tre       AUX       \n",
      "             ‚îÇ      ‚îî‚îÄ‚ñ∫‚îå‚î¨‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ consid√©r√©  xcomp      consid√©rer VERB      \n",
      "             ‚îÇ         ‚îÇ‚îÇ‚îÇ  ‚îå‚îÄ‚îÄ‚ñ∫ comme      case       comme      ADP       \n",
      "             ‚îÇ         ‚îÇ‚îÇ‚îÇ  ‚îÇ‚îå‚îÄ‚ñ∫ un         det        un         DET       \n",
      "             ‚îÇ         ‚îÇ‚îÇ‚îî‚îÄ‚ñ∫‚îî‚î¥‚îÄ‚îÄ s√©isme     obl:mod    s√©isme     NOUN      \n",
      "             ‚îÇ         ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ,          punct      ,          PUNCT     \n",
      "             ‚îÇ         ‚îÇ    ‚îå‚îÄ‚îÄ‚ñ∫ √†          case       √†          ADP       \n",
      "             ‚îÇ         ‚îÇ    ‚îÇ‚îå‚îÄ‚ñ∫ leur       det        leur       DET       \n",
      "             ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚ñ∫‚îî‚î¥‚îÄ‚îÄ √©chelle    obl:mod    √©chelle    NOUN      \n",
      "             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ .          punct      .          PUNCT     \n"
     ]
    }
   ],
   "source": [
    "import explacy\n",
    "explacy.print_parse_info(nlp, \"Depuis que Google a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Depuis que Google a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")\n",
    "root = [token for token in doc if token.head == token][0]\n",
    "objs = [tok for tok in root.rights if tok.dep_ in (\"obl:arg\", \"obj\", \"iobj\")]\n",
    "for obj in objs:\n",
    "    for descendant in obj.subtree:\n",
    "        print(descendant.text_with_ws, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matching par r√®gle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy a une classe `Matcher` qui permet de rep√©rer des tokens ou des suites de tokens √† l'aide de patrons (*pattern*). Ces patrons peuvent porter sur la forme des tokens ou leurs attributs (pos, ent).  \n",
    "On peut aussi utiliser des cat√©gories comme `IS_ALPHA` ou `IS_NUM`, voir la [doc](https://spacy.io/usage/rule-based-matching#adding-patterns-attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 8 en taille XL\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"en\"}, {\"LOWER\": \"taille\"}, {\"IS_ALPHA\": True, \"IS_UPPER\": True}]\n",
    "# en taille + lettres en maj\n",
    "matcher.add(\"tailles\", [pattern])\n",
    "\n",
    "doc = nlp(\"Ce mod√®le est aussi disponible en taille XL ; je vous le conseille.\")\n",
    "matches = matcher(doc)\n",
    "for _, start, end in matches:\n",
    "    #string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√áa fonctionne pour les s√©quences comme ¬´ en taille M ¬ª ou ¬´ en taille XL ¬ª mais pas pour ¬´ vous l'avez en XL ? ¬ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"vous l'avez en XL ?\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut essayer d'am√©liorer les r√®gles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en XL\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_1 = [{\"LOWER\": \"en\"}, {\"LOWER\": \"taille\"}, {\"IS_ALPHA\": True, \"IS_UPPER\": True}]\n",
    "pattern_2 = [{\"LOWER\": \"en\"}, {\"IS_ALPHA\": True, \"IS_UPPER\": True}]\n",
    "matcher.add(\"tailles\", [pattern_1, pattern_2])\n",
    "# r√®gle avec deux patterns\n",
    "\n",
    "doc = nlp(\"vous l'avez en XL ?\")\n",
    "matches = matcher(doc)\n",
    "for _, start, end in matches:\n",
    "    #string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou encore :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9364735015326875510 tailles 3 5 en XL\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "sizes = ['XS', 'S', 'M', 'L', 'XL']\n",
    "pattern_1 = [{\"LOWER\": \"en\"}, {\"LOWER\": \"taille\"}, {\"TEXT\": {\"IN\": sizes}}]\n",
    "pattern_2 = [{\"LOWER\": \"en\"}, {\"TEXT\": {\"IN\": sizes}}]\n",
    "matcher.add(\"tailles\", [pattern_1, pattern_2])\n",
    "# r√®gle avec deux patterns\n",
    "\n",
    "doc = nlp(\"vous l'avez en XL ?\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä vous\n",
    "\n",
    "Dans `files/Le_Ventre_de_Paris-short.txt`, trouver les s√©quences pronom - le lemme 'vendre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PRON\"}, {\"LEMMA\": \"vendre\"}]\n",
    "matcher.add(\"pro_vendre\", [pattern])\n",
    "\n",
    "doc = \"\"\n",
    "with open('files/Le_Ventre_de_Paris-short.txt') as input_data:\n",
    "    doc = nlp(input_data.read())\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elle vend\n",
      "on vendait\n",
      "qui vendait\n",
      "qui vendaient\n",
      "elle vendait\n",
      "Il vendrait\n"
     ]
    }
   ],
   "source": [
    "for _, start, end in matches:\n",
    "    #string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dependency Matcher :¬†extraction de patrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depuis la v3, Spacy a ajout√© un *Dependancy Matcher* qui permet de faire de l'extraction de patrons syntaxiques. Il est maintenant possible de faire porter des requ√™tes sur l'arbre syntaxique et non plus seulement sur la s√©quence des tokens.  \n",
    "Ce dispositif utilise [Semgrex](https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.html), la syntaxe utilis√©e dans Tgrep et Tregex, les outils de requ√™te sur Treebank de Stanford.\n",
    "\n",
    "Voir la [documentation](https://spacy.io/usage/rule-based-matching#dependencymatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ventre_short = \"\"\n",
    "with open('files/Le_Ventre_de_Paris-short.txt') as input_f:\n",
    "    ventre_short = input_f.read()\n",
    "doc = nlp(ventre_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vend\n",
      "vendant\n",
      "vendait\n",
      "vendait\n",
      "vendaient\n",
      "vendaient\n",
      "vend\n",
      "vendu\n",
      "vendu\n",
      "vendre\n",
      "vendait\n",
      "vendu\n",
      "vendais\n",
      "vendu\n",
      "vendrait\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "pattern = [\n",
    "  {\n",
    "    \"RIGHT_ID\": \"vendre\",    \n",
    "    \"RIGHT_ATTRS\": {\"LEMMA\": \"vendre\"}\n",
    "  }\n",
    "]\n",
    "matcher.add(\"VENDRE\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for m_id, t_ids in matches:\n",
    "    for t_id in t_ids:\n",
    "        print(doc[t_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbe, sujet, objet :¬† acheta -> il -> derniers\n",
      "objet complet :¬† ses deux derniers sous de pain\n",
      "Phrase compl√©te :¬† Mais, √† Vernon, il acheta ses deux derniers sous de pain.\n",
      "\n",
      "verbe, sujet, objet :¬† achetait -> elle -> qu‚Äô\n",
      "objet complet :¬† qu‚Äô\n",
      "Phrase compl√©te :¬† J‚Äô√©tais gamine, qu‚Äôelle achetait d√©j√† ses navets √† mon p√®re.\n",
      "\n",
      "verbe, sujet, objet :¬† achetait -> elle -> navets\n",
      "objet complet :¬† ses navets √† mon p√®re\n",
      "Phrase compl√©te :¬† J‚Äô√©tais gamine, qu‚Äôelle achetait d√©j√† ses navets √† mon p√®re.\n",
      "\n",
      "verbe, sujet, objet :¬† vendait -> on -> fruits\n",
      "objet complet :¬† des fruits\n",
      "Phrase compl√©te :¬† \n",
      "\n",
      "Derri√®re lui, sur le carreau de la rue Rambuteau, on vendait des fruits.\n",
      "\n",
      "verbe, sujet, objet :¬† vendaient -> qui -> bottes\n",
      "objet complet :¬† des bottes de foug√®re et des paquets de feuilles de vigne , bien r√©guliers , attach√©s par quarterons\n",
      "Phrase compl√©te :¬† Ils s‚Äôarr√™t√®rent curieusement devant des femmes qui vendaient des bottes de foug√®re et des paquets de feuilles de vigne, bien r√©guliers, attach√©s par quarterons.\n",
      "\n",
      "verbe, sujet, objet :¬† vend -> Lui -> volaille\n",
      "objet complet :¬† toute la volaille qu‚Äô il veut\n",
      "Phrase compl√©te :¬† Lui, vend toute la volaille qu‚Äôil veut‚Ä¶\n",
      "\n",
      "verbe, sujet, objet :¬† achetait -> il -> morceau\n",
      "objet complet :¬† un morceau de dinde ou un morceau d‚Äô oie de douze\n",
      "Phrase compl√©te :¬† Quand Florent rentrait trop tard pour faire cuire quelque bout de viande, il achetait en bas un morceau de dinde ou un morceau d‚Äôoie de douze sous.\n",
      "\n",
      "verbe, sujet, objet :¬† vendu -> Il -> mobilier\n",
      "objet complet :¬† le pauvre mobilier de la rue Royer\n",
      "Phrase compl√©te :¬† Il avait vendu le pauvre mobilier de la rue Royer-Collard, et en gardait l‚Äôargent, quarante et quelques francs, pour que ce farceur de Quenu, disait-il, ne le jet√¢t pas par les fen√™tres.\n",
      "\n",
      "verbe, sujet, objet :¬† vendait -> elle -> o√π\n",
      "objet complet :¬† o√π\n",
      "Phrase compl√©te :¬† Lorsqu‚Äôelle le vit s‚Äô√©tablir aux Halles, √† deux pas du pavillon o√π elle vendait du beurre, des fromages et des ≈ìufs, elle l‚Äôaccusa d‚Äôavoir ¬´ invent√© √ßa pour la taquiner et lui porter mauvaise chance.\n",
      "\n",
      "verbe, sujet, objet :¬† vendait -> elle -> beurre\n",
      "objet complet :¬† du beurre , des fromages et des ≈ìufs\n",
      "Phrase compl√©te :¬† Lorsqu‚Äôelle le vit s‚Äô√©tablir aux Halles, √† deux pas du pavillon o√π elle vendait du beurre, des fromages et des ≈ìufs, elle l‚Äôaccusa d‚Äôavoir ¬´ invent√© √ßa pour la taquiner et lui porter mauvaise chance.\n",
      "\n",
      "verbe, sujet, objet :¬† achet√© -> Je -> vous\n",
      "objet complet :¬† vous\n",
      "Phrase compl√©te :¬† Je vous en ai achet√© avant-hier, du boudin‚Ä¶\n",
      "\n",
      "verbe, sujet, objet :¬† vendu -> vous -> m‚Äô\n",
      "objet complet :¬† m‚Äô\n",
      "Phrase compl√©te :¬† Elle se courba, les poings sur son comptoir ; et, d‚Äôune voix un peu rauque :\n",
      "\n",
      "‚Äî Dites donc, la semaine derni√®re, quand vous m‚Äôavez vendu cette paire de soles, vous savez, est-ce que je suis all√©e vous dire qu‚Äôelles √©taient pourries devant le monde !\n",
      "\n",
      "verbe, sujet, objet :¬† vendu -> vous -> paire\n",
      "objet complet :¬† cette paire de soles\n",
      "Phrase compl√©te :¬† Elle se courba, les poings sur son comptoir ; et, d‚Äôune voix un peu rauque :\n",
      "\n",
      "‚Äî Dites donc, la semaine derni√®re, quand vous m‚Äôavez vendu cette paire de soles, vous savez, est-ce que je suis all√©e vous dire qu‚Äôelles √©taient pourries devant le monde !\n",
      "\n",
      "verbe, sujet, objet :¬† vendrait -> Il -> semelles\n",
      "objet complet :¬† des semelles de bottes\n",
      "Phrase compl√©te :¬† Il vendrait des semelles de bottes pour des paires de soles.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "pattern = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"vendre\",    \n",
    "        \"RIGHT_ATTRS\": {\"LEMMA\": {\"IN\": [\"vendre\", \"acheter\"]}}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"vendre\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"sujet\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"},  \n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"vendre\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"objet\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"obj\", \"iobj\", \"obl\"]}},  \n",
    "    }\n",
    "]\n",
    "matcher.add(\"VENDRE\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for m_id, t_ids in matches:\n",
    "    print(\"verbe, sujet, objet :¬†\", \" -> \".join([doc[t_id].text for t_id in t_ids]))\n",
    "    print(\"objet complet :¬†\", \" \".join([t.text for t in doc[t_ids[2]].subtree]))\n",
    "    print(\"Phrase compl√©te :¬†\", doc[t_ids[0]].sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è √Ä vous\n",
    "\n",
    "Ajouter une r√®gle au motif pour trouver aussi l'objet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapter les traitements de Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. re-tokenisation\n",
    "\n",
    "- voir [https://spacy.io/usage/linguistic-features#retokenization](https://spacy.io/usage/linguistic-features#retokenization)\n",
    "\n",
    "Dans l'exemple qui suit ¬´ quer-cra ¬ª sera tokeniz√© √† tort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pour', 'ADP', 'pour'), ('les', 'DET', 'le'), ('bons', 'ADJ', 'bon'), ('bails', 'NOUN', 'bail'), ('√ßa', 'PRON', 'cela'), ('va', 'VERB', 'aller'), ('grave', 'ADJ', 'grave'), ('quer', 'VERB', 'quer'), ('-', 'PUNCT', '-'), ('cra', 'PROPN', 'cra')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Pour les bons bails √ßa va grave quer-cra\")\n",
    "print([(tok.text, tok.pos_, tok.lemma_)for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pour', 'ADP'), ('les', 'DET'), ('bons', 'ADJ'), ('bails', 'NOUN'), ('√ßa', 'PRON'), ('va', 'VERB'), ('grave', 'ADJ'), ('quer-cra', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[7:], attrs={\"LEMMA\": \"quer-cra\", \"POS\": \"NOUN\"})\n",
    "print([(tok.text, tok.pos_) for tok in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention ici c‚Äôest l‚Äôobjet doc qui est modifi√©, le r√©sultat mais pas le traitement. Nous allons voir comment faire pour modifier le traitement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modification de la tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pour', 'ADP', 'pour'), ('les', 'DET', 'le'), ('bons', 'ADJ', 'bon'), ('bails', 'NOUN', 'bail'), ('√ßa', 'PRON', 'cela'), ('va', 'VERB', 'aller'), ('grave', 'ADJ', 'grave'), ('quer-cra', 'PROPN', 'quer-cra')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH, LEMMA, POS, TAG\n",
    "\n",
    "special_case = [{ORTH: \"quer-cra\"}]\n",
    "nlp.tokenizer.add_special_case(\"quer-cra\", special_case)\n",
    "doc = nlp(\"Pour les bons bails √ßa va grave quer-cra\")\n",
    "print([(tok.text, tok.pos_, tok.lemma_) for tok in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien modifi√© la tokenisation dans le mod√®le `nlp`. Cela n'affecte pas par contre l'√©tiquetage en POS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entit√©s nomm√©es :¬†traitement par r√®gles\n",
    " - Voir [https://spacy.io/usage/rule-based-matching#entityruler](https://spacy.io/usage/rule-based-matching#entityruler)\n",
    " \n",
    "Spacy offre aussi un m√©canisme de traitement par r√®gle pour les entit√©s nomm√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant :  [('Chrome', 'MISC'), ('Criteo', 'MISC'), ('LiveRamp', 'ORG'), ('Index Exchange', 'MISC')]\n",
      "Apr√®s :  [('machin', 'ORG'), ('Chrome', 'ORG'), ('Criteo', 'ORG'), ('LiveRamp', 'ORG'), ('Index Exchange', 'MISC')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "#nlp = spacy.load('fr_core_news_md')\n",
    "doc = nlp(\"Depuis que machin a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")\n",
    "print(\"Avant : \", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", config={'overwrite_ents':True})\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Chrome\"},\n",
    "            {\"label\":\"ORG\", \"pattern\":\"machin\"},\n",
    "    {\"label\":\"ORG\", \"pattern\":\"Criteo\"},\n",
    "    {\"label\":\"ORG\",\"pattern\":\"LiveRamp\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"Depuis que machin a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")\n",
    "print(\"Apr√®s : \", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entit√©s nomm√©es : entra√Ænement\n",
    "\n",
    "Ici nous avons un exemple sur les entit√©s nomm√©es mais la proc√®dure d'entra√Ænement fonctionne pour d'autres niveaux d'annotations (pos, d√©pendance). Voir la doc :¬†https://spacy.io/usage/training\n",
    "\n",
    "  - pr√©paration des donn√©es\n",
    "  \n",
    "√âvidemment nous aurons besoin de donn√©es annot√©es pour les phases d'entra√Ænement et de test.  \n",
    "Nous conserverons le tagset utilis√©s dans le fran√ßais (LOC, MISC, ORG, PER) pour annoter manuellement des extraits de la page Wikipedia https://fr.wikipedia.org/wiki/Personnages_de_Mario  \n",
    "Nous travaillerons sur 5 petits fichiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "luigi.txt  mario.txt  peach.txt  toad.txt  yoshi.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls train/txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mario est le h√©ros du Royaume Champignon. Malgr√© son apparence banale de plombie\r\n",
      "r, Mario poss√®de une tr√®s grande force et des capacit√©s de sauts incroyables. Sa\r\n",
      " rapidit√© lui conf√®re une grande habilet√© au combat. Mario est le personnage pri\r\n",
      "ncipal de la quasi-totalit√© des jeux de la s√©rie Super Mario et des jeux \"Guerri\r\n",
      "c Stats\" Il est aussi l'ennemi jur√© de Bowser. Il peut se transformer de plusieu\r\n",
      "rs fa√ßons apr√®s avoir re√ßu des objets tels que le Super Champignon, la Fleur de \r\n",
      "feu et la Super √©toile, il est courageux et peut venir √† bout de n'importe quell\r\n",
      "e situation! Il est apparu pour la premi√®re fois dans Donkey Kong en 1981, au d√©\r\n",
      "part il √©tait appel√© sous le nom de Jumpman (homme qui saute); il est rebaptis√© \r\n",
      "Mario dans Donkey Kong Jr. sorti en arcade en 1982. \r\n"
     ]
    }
   ],
   "source": [
    "!more train/txt/mario.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces fichiers doivent √™tre tokeniz√©s puis annot√©s au format IOB. Voir l'exemple https://github.com/explosion/spaCy/blob/master/extra/example_data/ner_example_data/ner-token-per-line.iob\n",
    "\n",
    "Puis les fichiers seront convertis √† l'aide de la commande `convert` (https://spacy.io/api/cli#convert).  \n",
    "Vous devrez avoir 4 fichiers dans un dossier `train_dir` et un fichier dans `dev_dir`.   \n",
    "Exemple avec un output dans le dossier `dev_dir`‚ÄØ:  \n",
    "`python -m spacy convert toad.iob dev_dir --converter ner`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - configuration\n",
    " \n",
    " Dans la version 3.0, Spacy utilise un fichier de configuration dont le format est d√©fini dans Thinc (https://thinc.ai/docs/usage-config). Le plus simple est d'utiliser le widget de la doc pour d√©finir vos param√®tres principaux : https://spacy.io/usage/training#quickstart\n",
    " \n",
    " Puis vous utilisez la commande ci-dessous pour g√©n√©rer votre fichier de configuration : \n",
    " `python -m spacy init fill-config base_config.cfg config.cfg`\n",
    " \n",
    "Il y a quantit√© de param√®tres √† d√©finir dans ce fichier de config √©videmment. `init` utilise des valeurs par d√©faut que vous pourrez modifier comme vous voulez.  \n",
    "Retenez toutefois que vous pouvez choisir soit d'entra√Æner un mod√®le ou un des composants du mod√®le *from scratch*, soit de modifier les poids d'un mod√®le existant.\n",
    "\n",
    "From scratch : \n",
    "```\n",
    "[components.ner]\n",
    "factory = \"ner\"\n",
    "```\n",
    "\n",
    "Depuis un mod√®le :¬†\n",
    "```\n",
    "[components.ner]\n",
    "source = \"fr_core_news_md\"    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - entra√Ænement\n",
    "  \n",
    "  Une fois que vous avez les donn√©es annot√©es au bon format et le fichier de config, vous pouvez lancer l'entra√Ænement.\n",
    "  \n",
    " `python -m spacy train config.cfg -o model --paths.train ./train_dir --paths.dev ./dev_dir`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - √©valuation\n",
    "  \n",
    "  Spacy propose √©galement un outil d'√©valuation qui vous permettra de comparer les performances des mod√®les que vous avez g√©n√©r√©. Les m√©triques sont choisies en fonction du/des types d'annotations du mod√®le. Pour les entit√©s nomm√©es on a : Pr√©cision, Rappel, F-Mesure.\n",
    "  \n",
    "`python -m spacy evaluate model/model-best/ dev_dir/toad.spacy`\n",
    "\n",
    "`python -m spacy evaluate fr_core_news_md dev_dir/toad.spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     -    \n",
      "NER P   71.43\n",
      "NER R   83.33\n",
      "NER F   76.92\n",
      "SPEED   9261 \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "            P        R        F\n",
      "PER    100.00   100.00   100.00\n",
      "LOC    100.00   100.00   100.00\n",
      "MISC    33.33    50.00    40.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy evaluate train/model_2/model-best/ train/dev_dir/toad.spacy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
