{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outils-corpus 5\n",
    "## [Spacy](https://spacy.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Biblioth√®que logicielle de TAL √©crite en Python (et Cython)\n",
    "- √âtiquetage POS, lemmatisation, analyse syntaxique, entit√©s nomm√©es, word embedding\n",
    "- Usage de mod√®les neuronaux\n",
    "- Int√©gration ais√©e de biblioth√®ques de deep learning\n",
    "- v2.2.3 ([github](https://github.com/explosion/spaCy))\n",
    "- Licence MIT (Open Source) pour le code\n",
    "    - Licences ouvertes diverses pour les mod√®les\n",
    "- Produit de la soci√©t√© [explosion.ai](https://explosion.ai/). Fond√© par :¬†Matthew Honnibal ([@honnibal](https://twitter.com/honnibal)) et Ines Montani ([@_inesmontani](https://twitter.com/_inesmontani))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pourquoi Spacy ?\n",
    "\n",
    "- C'est du Python üôå üéâ\n",
    "- Plut√¥t simple √† prendre en main\n",
    "- Tr√®s bien document√©, √† notre avis. D'ailleurs plut√¥t que ce notebook, suivez l'excellent tutorial d'Ines Montani : [https://course.spacy.io/](https://course.spacy.io/)\n",
    "- Couvre les traitements d'une cha√Æne de TAL typique\n",
    "- Pas mal utilis√© dans l'industrie\n",
    "- MAIS ce n'est pas forc√©ment l'outil qui donne les meilleurs r√©sultats pour le fran√ßais\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy et les autres\n",
    "\n",
    "Spacy est *un* des frameworks de TAL disponibles\n",
    "\n",
    "- [NLTK](http://www.nltk.org/) :¬†python, orient√© p√©dagogie, pas de mod√®les neuronaux inclus mais se combine bien avec TensorFlow, PyTorch ou AlleNLP\n",
    "- [Stanford Core¬†NLP](https://stanfordnlp.github.io/stanfordnlp/) :¬†java, mod√®les pour 53 langues (UD), r√©solution de la cor√©ference.\n",
    "    <small>[https://github.com/explosion/spacy-stanfordnlp](https://github.com/explosion/spacy-stanfordnlp) permet d'utiliser les mod√®les de CoreNLP avec Spacy</small>\n",
    "- [Stanza](https://stanfordnlp.github.io/stanza/) :¬†python, nouveau framework de Stanford, mod√®les neuronaux entra√Æn√©s sur donn√©es UD\n",
    "- [TextBlob](https://textblob.readthedocs.io/en/dev/)\n",
    "- [DKPro](https://dkpro.github.io/)\n",
    "- [flair](https://github.com/zalandoresearch/flair) : le framework de Zalando, tr√®s bonnes performances en reconnaissance d'entit√©s nomm√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## installation\n",
    "\n",
    "dans un terminal\n",
    "```bash\n",
    "python3 -m pip install --user spacy \n",
    "#ou pip install --user spacy\n",
    "```\n",
    "- installation du mod√®le fran√ßais\n",
    "```bash\n",
    "python3 -m spacy download fr_core_news_sm\n",
    "#ou python3 -m spacy download fr_core_news_md\n",
    "```\n",
    "- v√©rification\n",
    "```bash\n",
    "python3 -m spacy validate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mod√®les\n",
    "\n",
    "- 10 langues :¬†anglais, allemand, fran√ßais, espagnol, portugais, italien, n√©erlandais, grec, norv√©gien, lituanien + mod√®le multi langues\n",
    "- 2 mod√®les pour le fran√ßais\n",
    "    - fr_core_news_sm (tagger, parser, ner) 14 Mo\n",
    "    - fr_core_news_md (tagger, parser, ner, vectors) 84 Mo\n",
    "- mod√®les `fr` appris sur les corpus [Sequoia](https://deep-sequoia.inria.fr/fr/) et [WikiNer](https://figshare.com/articles/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage\n",
    "\n",
    "- *si vous voulez utiliser Spacy prenez le temps de lire la [documentation](https://spacy.io/usage), ici ce ne sera qu'un coup d'≈ìil incomplet*\n",
    "- un mod√®le est une instance de la classe `Language`, il est adapt√© √† une langue en particulier\n",
    "- un mod√®le incorpore un vocabulaire, des poids, des vecteurs de mots, une configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage\n",
    "\n",
    "- le traitement fonctionne avec un [*pipeline*](https://spacy.io/usage/spacy-101#pipelines) pour convertir un texte en objet `Doc` (texte annot√©)\n",
    "- par d√©faut `tokenizer` > `tagger` > `parser` > `ner` > `‚Ä¶`\n",
    "- l'utilisateur peut ajouter des √©tapes ou en retrancher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_sm', disable=[\"parser\", \"ner\"])\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retour au pipeline par d√©faut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_sm')\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage\n",
    "\n",
    " - Un objet `Doc` est une s√©quence d'objets `Token` (voir l'[API](https://spacy.io/api/token))\n",
    " - Le texte d'origine est d√©coup√© en phrases, tokeniz√©, annot√© en POS, lemme, syntaxe (d√©pendance) et en entit√©s nomm√©es (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì tokenization\n",
    "\n",
    "La tokenization de Spacy est non-destructive. Vous pouvez d√©couper un texte en tokens et le restituer dans sa forme originale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"L‚ÄôOrganisation des Nations unies (ONU) a lanc√© mardi un appel d‚Äôurgence pour lever des dizaines de millions de dollars afin de prot√©ger les r√©fugi√©s vuln√©rables face √† la propagation du nouveau coronavirus.\")\n",
    "for tok in doc:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in doc:\n",
    "    print(tok.text_with_ws, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì √©tiquetage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Dans les derniers jours de mai 1793, un des bataillons parisiens amen√©s en Bretagne par Santerre fouillait le redoutable bois de la Saudraie en Astill√©. On n'√©tait pas plus de trois cents, car le bataillon √©tait d√©cim√© par cette rude guerre.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.tag_, token.lemma_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour traiter plusieurs textes, vous pouvez utiliser [nlp.pipe](https://spacy.io/api/language#pipe)\n",
    "\n",
    "\n",
    "```python\n",
    "docs = list(nlp.pipe(texts))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì NER\n",
    "\n",
    "Acc√®s direct aux entit√©s de l'objet `Doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Le pr√©sident Xi Jinping a affirm√© que la propagation du coronavirus √©tait ¬´‚ÄØpratiquement jugul√©e‚ÄØ¬ª. Il s‚Äôest d‚Äôailleurs rendu pour la premi√®re fois √† Wuhan, la capitale de la province du Hubei, le berceau du Covid-19.')\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage ‚Äì analyse syntaxique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"dep\", jupyter=True, options={'distance':80})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`noun_chunks` permet de r√©cup√©rer les syntagmes nominaux d'un document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\" Et lorsque les Am√©ricains de PBS, les Chinois de Tencent et FranceT√©l√©visions se joignent au projet, le r√©sultat final est, logiquement, √† la hauteur des esp√©rances. Gr√¢ce √† la pr√©cision des images, des centaines de satellites √©quip√©s de capteurs surpuissants et de cam√©ras tournant en orbite autour de notre plan√®te, ce documentaire ne se contente pas d‚Äôoffrir des images √† couper le souffle.\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(f\"{chunk.text} ({chunk.root.text})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi r√©cup√©rer la t√™te syntaxique et ses d√©pendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = [token for token in doc if token.head == token][0]\n",
    "subjects = [tok for tok in root.lefts if tok.dep_ == \"nsubj\"]\n",
    "subject = subjects[0]\n",
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    for descendant in subject.subtree:\n",
    "        print(descendant.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √Ä¬†vous\n",
    "\n",
    "1. Trouver et afficher l'objet de la phrase :¬†¬´ Depuis que Google a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle. ¬ª\n",
    "\n",
    "2. Afficher les entit√©s nomm√©es et leur cat√©gorie de cette m√™me phrase. Y-a-t'il des erreurs selon vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Depuis que Google a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")\n",
    "#displacy.render(doc, style=\"dep\", jupyter=True)\n",
    "root = [token for token in doc if token.head == token][0]\n",
    "objs = [tok for tok in root.rights if tok.dep_ in (\"obl\", \"obj\", \"iobj\")]\n",
    "#objs\n",
    "for obj in objs:\n",
    "    for descendant in obj.subtree:\n",
    "        print(descendant.text_with_ws, end=\"\")\n",
    "    print(\" \", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapter les traitements de Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. re-tokenisation\n",
    "\n",
    "- voir [https://spacy.io/usage/linguistic-features#retokenization](https://spacy.io/usage/linguistic-features#retokenization)\n",
    "\n",
    "Dans l'exemple qui suit ¬´ quer-cra ¬ª sera tokeniz√© √† tort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Pour les bons bails √ßa va grave quer-cra\")\n",
    "print([(tok.text, tok.lemma_)for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[7:], attrs={\"LEMMA\": \"quer-cra\", \"POS\": \"NOUN\"})\n",
    "print([(tok.text, tok.pos_) for tok in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention ici c‚Äôest l‚Äôobjet doc qui est modifi√©, le r√©sultat mais pas le traitement. Nous allons voir comment faire pour modifier le traitement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modification de la tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH, LEMMA, POS, TAG\n",
    "\n",
    "special_case = [{ORTH: \"quer-cra\", LEMMA: \"quer-cra\", POS: \"VERB\"}]\n",
    "nlp.tokenizer.add_special_case(\"quer-cra\", special_case)\n",
    "doc = nlp(\"Pour les bons bails √ßa va grave quer-cra\")\n",
    "print([(tok.text, tok.pos_, tok.lemma_) for tok in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien modifi√© la tokenisation dans le mod√®le `nlp`. Cela n'affecte pas par contre l'√©tiquetage en POS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Matching par r√®gle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy a une classe `Matcher` qui permet de rep√©rer des tokens ou des suites de tokens √† l'aide de patrons (*pattern*). Ces patrons peuvent porter sur la forme des tokens ou leurs attributs (pos, ent).  \n",
    "On peut aussi utiliser des cat√©gories comme `IS_ALPHA` ou `IS_NUM`, voir la [doc](https://spacy.io/usage/rule-based-matching#adding-patterns-attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"en\"}, {\"LOWER\": \"taille\"}, {\"IS_ALPHA\": True, \"IS_UPPER\": True}]\n",
    "# en taille + lettres en maj\n",
    "matcher.add(\"tailles\", None, pattern)\n",
    "\n",
    "doc = nlp(\"Ce mod√®le est aussi disponible en taille M\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√áa fonctionne pour les s√©quences comme ¬´ en taille M ¬ª ou ¬´ en taille XL ¬ª mais pas pour ¬´ vous l'avez en XL ? ¬ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"vous l'avez en XL ?\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut essayer d'am√©liorer les r√®gles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_1 = [{\"LOWER\": \"en\"}, {\"LOWER\": \"taille\"}, {\"IS_ALPHA\": True, \"IS_UPPER\": True}]\n",
    "pattern_2 = [{\"LOWER\": \"en\"}, {\"IS_ALPHA\": True, \"IS_UPPER\": True}]\n",
    "matcher.add(\"tailles\", None, pattern_1, pattern_2)\n",
    "# r√®gle avec deux patterns\n",
    "\n",
    "doc = nlp(\"vous l'avez en XL ?\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou encore :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "sizes = ['XS', 'S', 'M', 'L', 'XL']\n",
    "pattern_1 = [{\"LOWER\": \"en\"}, {\"LOWER\": \"taille\"}, {\"TEXT\": {\"IN\": sizes}}]\n",
    "pattern_2 = [{\"LOWER\": \"en\"}, {\"TEXT\": {\"IN\": sizes}}]\n",
    "matcher.add(\"tailles\", None, pattern_1, pattern_2)\n",
    "# r√®gle avec deux patterns\n",
    "\n",
    "doc = nlp(\"vous l'avez en XL ?\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√Ä vous : \n",
    "√âcrivez un ou des patrons pour rep√©rer les tournures du type ¬´ j‚Äôveux du L ¬ª, ¬´ j‚Äôprends le V ¬ª dans les phrases :\n",
    "\n",
    " - ¬´ J‚Äôveux du L, j‚Äôveux du V, j‚Äôveux du G, pour d√©ssaper ta racli ¬ª\n",
    " - ¬´ J‚Äôme repose sur les faits, ouais bient√¥t j‚Äôme refais, j‚Äôte laisse L, j‚Äôprends le V, l‚Äôenfer m‚Äôest r√©serv√©‚ÄØ¬ª\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entit√©s nomm√©es :¬†traitement par r√®gles\n",
    " - Voir [https://spacy.io/usage/rule-based-matching#entityruler](https://spacy.io/usage/rule-based-matching#entityruler)\n",
    " \n",
    "Spacy offre aussi un m√©canisme de traitement par r√®gle pour les entit√©s nomm√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(\"Depuis que machin a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")\n",
    "print(\"Avant : \", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "ruler = EntityRuler(nlp, overwrite_ents=True)\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Chrome\"},\n",
    "            {\"label\":\"ORG\", \"pattern\":\"machin\"},\n",
    "    {\"label\":\"ORG\", \"pattern\":\"Criteo\"},\n",
    "    {\"label\":\"ORG\",\"pattern\":\"LiveRamp\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "doc = nlp(\"Depuis que machin a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")\n",
    "print(\"Apr√®s : \", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entit√©s nomm√©es ¬†: adaptation du mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy permet aussi d'adapter un mod√®le, de le mettre √† jour avec un entra√Ænement partiel.  \n",
    "C'est particuli√®rement int√©ressant puisque cela permet d'adapter un mod√®le g√©n√©rique √† un domaine particulier. Avec un volume de donn√©es limit√©es, sans devoir repartir √† z√©ro. C'est ce qu'on appelle le *transfert learning*.\n",
    "\n",
    "Bien √©videmment il faut constituer un petit corpus d'apprentissage. Soit √† la main, soit en s'aidant d'un `Matcher`, d'un `Phrase Matcher` ou d'un `Entity Ruler`. De combiner donc le traitement par r√®gle et l'apprentissage.\n",
    "\n",
    "[Explosion.ai](https://explosion.ai/), la soci√©t√© qui √©dite Spacy, propose un outil commercial nomm√© [Prodigy](https://prodi.gy/) pour constituer ais√©ment un corpus d'apprentissage et lancer les processus de *transfert learning*. \n",
    "\n",
    "Ici nous allons utiliser un jeu de donn√©es beaucoup tr√®s limit√©. Pour √™tre r√©aliste il en faudrait beaucoup plus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    (\"Criteo fonctionne gr√¢ce √† des data centers\", {\"entities\": [(0, 6, \"ORG\")]}),\n",
    "    (\"LiveRamp a rachet√© Criteo\", {\"entities\": [(0, 8, \"ORG\"), (18, 25, \"ORG\")]}),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le code qui suit, repris de la doc [ici](https://spacy.io/usage/examples#training-ner), nous allons utiliser la m√©thode `nlp.update` un certain nombre de fois (ici 100). √Ä¬†chaque it√©ration la fonction calcule une perte (p√©nalit√© li√©e aux mauvaises pr√©dictions), on garde le mod√®le qui a la plus faible perte.\n",
    "\n",
    "Vous trouverez √©galement ci-dessus les m√©thodes pour enregistrer un mod√®le et le charger en m√©moire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model :\n",
      "Entities [('Criteo', 'MISC')]\n",
      "Entities [('LiveRamp', 'MISC'), ('Criteo', 'MISC')]\n",
      "Losses {'ner': 3.1428039306629216}\n",
      "Losses {'ner': 1.9436554685493608}\n",
      "Losses {'ner': 2.0977062570183413}\n",
      "Losses {'ner': 1.5694014945547679}\n",
      "Losses {'ner': 4.264512949044257}\n",
      "Losses {'ner': 1.9725959087372757}\n",
      "Losses {'ner': 2.356550514016505}\n",
      "Losses {'ner': 1.3981396627777507}\n",
      "Losses {'ner': 0.059147988826914855}\n",
      "Losses {'ner': 0.5513820468881363}\n",
      "Losses {'ner': 0.13139400188854405}\n",
      "Losses {'ner': 0.27305141493796126}\n",
      "Losses {'ner': 0.1426036087373319}\n",
      "Losses {'ner': 0.04276248905330249}\n",
      "Losses {'ner': 0.00011435651585289808}\n",
      "Losses {'ner': 0.1366490641949916}\n",
      "Losses {'ner': 0.0007795512978328589}\n",
      "Losses {'ner': 3.82194560798188e-06}\n",
      "Losses {'ner': 0.0007613747879391263}\n",
      "Losses {'ner': 0.01826789357083669}\n",
      "Losses {'ner': 9.907136099073152e-06}\n",
      "Losses {'ner': 2.1407109930902523e-10}\n",
      "Losses {'ner': 8.311232454616402e-05}\n",
      "Losses {'ner': 0.000668206892865623}\n",
      "Losses {'ner': 0.0056022221200354595}\n",
      "Losses {'ner': 4.543129268005557e-05}\n",
      "Losses {'ner': 9.992528483241183e-06}\n",
      "Losses {'ner': 3.887820406573567e-06}\n",
      "Losses {'ner': 3.633402150838433e-05}\n",
      "Losses {'ner': 4.470732005914233e-06}\n",
      "Losses {'ner': 3.252966831091995e-07}\n",
      "Losses {'ner': 7.612355190644848e-07}\n",
      "Losses {'ner': 1.2692293382614833e-06}\n",
      "Losses {'ner': 3.8039414423732018e-06}\n",
      "Losses {'ner': 0.8204126067431332}\n",
      "Losses {'ner': 2.138936420662636e-08}\n",
      "Losses {'ner': 1.1562865399134849e-05}\n",
      "Losses {'ner': 2.843609443914401e-06}\n",
      "Losses {'ner': 4.493375607705907e-08}\n",
      "Losses {'ner': 3.3684653011241406e-06}\n",
      "Losses {'ner': 2.12413251734459e-07}\n",
      "Losses {'ner': 8.735717156027795e-10}\n",
      "Losses {'ner': 5.045910793890885e-07}\n",
      "Losses {'ner': 1.183816450801305e-06}\n",
      "Losses {'ner': 1.330260272934731e-10}\n",
      "Losses {'ner': 4.6240127087501825e-08}\n",
      "Losses {'ner': 1.514342267788657e-05}\n",
      "Losses {'ner': 1.4707295915850072e-06}\n",
      "Losses {'ner': 1.2416879760662714e-07}\n",
      "Losses {'ner': 2.9247563265500816e-10}\n",
      "Losses {'ner': 2.183791104217356e-05}\n",
      "Losses {'ner': 9.945101841005166e-09}\n",
      "Losses {'ner': 9.342273077296377e-09}\n",
      "Losses {'ner': 1.4014280445211018e-11}\n",
      "Losses {'ner': 6.021161717976069e-09}\n",
      "Losses {'ner': 1.640769816216291e-09}\n",
      "Losses {'ner': 3.166655483359441e-07}\n",
      "Losses {'ner': 1.0760126824143105e-09}\n",
      "Losses {'ner': 3.9245006325657176e-08}\n",
      "Losses {'ner': 2.740441344059641e-06}\n",
      "Losses {'ner': 6.977855033004493e-07}\n",
      "Losses {'ner': 1.9803249343166543e-09}\n",
      "Losses {'ner': 3.5878633472375874e-08}\n",
      "Losses {'ner': 1.7256983492680123e-10}\n",
      "Losses {'ner': 4.686518149902537e-09}\n",
      "Losses {'ner': 4.078670761556893e-11}\n",
      "Losses {'ner': 3.910760917837847e-05}\n",
      "Losses {'ner': 4.401397271778721e-08}\n",
      "Losses {'ner': 4.935509484106067e-07}\n",
      "Losses {'ner': 9.292175060928256e-08}\n",
      "Losses {'ner': 3.213716350599628e-10}\n",
      "Losses {'ner': 1.809545629376957e-08}\n",
      "Losses {'ner': 3.597366066956836e-09}\n",
      "Losses {'ner': 3.494614132301255e-09}\n",
      "Losses {'ner': 9.99586571277071e-10}\n",
      "Losses {'ner': 2.3332780682843954e-13}\n",
      "Losses {'ner': 1.0442276849156243e-05}\n",
      "Losses {'ner': 1.3837778095322422e-07}\n",
      "Losses {'ner': 0.005824319885486421}\n",
      "Losses {'ner': 1.5560800906613674e-11}\n",
      "Losses {'ner': 5.384492666911446e-10}\n",
      "Losses {'ner': 3.7890868873406697e-07}\n",
      "Losses {'ner': 2.033461317577165e-09}\n",
      "Losses {'ner': 1.2663357070782133e-11}\n",
      "Losses {'ner': 1.4265386323639986e-08}\n",
      "Losses {'ner': 6.709156167751088e-09}\n",
      "Losses {'ner': 2.8209913572628904e-11}\n",
      "Losses {'ner': 7.971477324401786e-07}\n",
      "Losses {'ner': 1.5806241482654266e-09}\n",
      "Losses {'ner': 1.4179809861624658e-11}\n",
      "Losses {'ner': 3.5134749310815537e-10}\n",
      "Losses {'ner': 9.54592116158097e-11}\n",
      "Losses {'ner': 1.4837106487717548e-07}\n",
      "Losses {'ner': 1.7361502794029722e-08}\n",
      "Losses {'ner': 2.873922360284773e-08}\n",
      "Losses {'ner': 2.6942791053721648e-06}\n",
      "Losses {'ner': 3.7022647426922727e-10}\n",
      "Losses {'ner': 7.144990249986274e-06}\n",
      "Losses {'ner': 8.224614202897005e-09}\n",
      "Losses {'ner': 5.9271219860007694e-08}\n",
      "Testing model : \n",
      "Entities [('Criteo', 'ORG')]\n",
      "Entities [('LiveRamp', 'ORG'), ('Criteo', 'ORG')]\n",
      "Saved model to .\n",
      "Loading from .\n",
      "Entities [('Criteo', 'ORG')]\n",
      "Entities [('LiveRamp', 'ORG'), ('Criteo', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "print(\"Original model :\")\n",
    "for text, _ in TRAIN_DATA:\n",
    "    doc = nlp(text)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "# add labels\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    for itn in range(100):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(\n",
    "                texts,  # batch of texts\n",
    "                annotations,  # batch of annotations\n",
    "                drop=0.5,  # dropout - make it harder to memorise data\n",
    "                losses=losses,\n",
    "            )\n",
    "        print(\"Losses\", losses)\n",
    "\n",
    "# test the trained model\n",
    "print(\"Testing model : \")\n",
    "for text, _ in TRAIN_DATA:\n",
    "    doc = nlp(text)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "# save model to output directory\n",
    "output_dir = Path(\"./\")\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)\n",
    "\n",
    "# test the saved model\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "for text, _ in TRAIN_DATA:\n",
    "    doc = nlp2(text)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Chrome ORG\n",
      "Criteo ORG\n",
      "LiveRamp ORG\n",
      "Index Exchange ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp2(\"Depuis que Google a annonc√© son intention de stopper d'ici deux ans les cookies tiers sur Chrome , son moteur de recherche qui est utilis√© par plus de 60 % de la population mondiale connect√©e, les Criteo, LiveRamp et autres Index Exchange se pr√©parent √† ce qui peut √™tre consid√©r√© comme un s√©isme, √† leur √©chelle.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
